{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRDe0amzmxx5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3CsklH1_Jo"
      },
      "source": [
        "\n",
        "## Train/Test Split\n",
        "\n",
        "We split the data BEFORE calculating fairness labels or performing scaling.\n",
        "This ensures that the benchmarks for 'Fairly Priced' are derived only from the training distribution, preventing information from the test set leaking into the training process (Data Leakage).\n",
        "\n",
        "## Define features (X) and the continuous target (y)\n",
        "'y' is the raw price, which we will use to derive classification labels later.\n",
        "Perform the split (80% Train, 20% Test)\n",
        "We use random_state=42 for reproducibility of results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4YiJVTJ0I8u"
      },
      "outputs": [],
      "source": [
        "# Train / Test Split (FIRST – prevents leakage)\n",
        "X = data.drop(columns=['price'])\n",
        "y = data['price']  # used ONLY for label creation, never as a feature\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cj9kqxI21Jx"
      },
      "source": [
        "\n",
        "# Section 3: MARKET CONTEXT FEATURES (FEATURE ENGINEERING)\n",
        "\n",
        "We calculate the price distribution (Quartiles) per locality to establish \"Local Market Norms.\" This allows the model to judge a property's price relative to its specific neighborhood rather than the entire country.\n",
        "\n",
        " 1. Compute location-level price statistics using ONLY Training Data. We use lambda functions for 25th and 75th percentiles to define price boundaries.\n",
        " 2. Integrate these benchmarks back into the datasets. We merge into BOTH train and test.\n",
        "Note: X_test receives benchmarks derived from X_train to simulate a real-world scenario where the model encounters a new listing in a known locality.\n",
        "  3. Handle Missing Values. If a locality in the Test set was NOT in the Training set, it will have NaNs.  We fill these with the global medians calculated from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alB-NPfn21sY"
      },
      "outputs": [],
      "source": [
        "# Market Context Features (Training Data Only)\n",
        "# Compute location-level price statistics used for:\n",
        "# - class label creation\n",
        "# - contextual features\n",
        "# These features capture local market price context and are used for:\n",
        "# - price class label creation\n",
        "# - relative pricing features\n",
        "# Statistics are computed ONLY on training data to Prevent data leakage from test set\n",
        "\n",
        "\n",
        "location_price_stats = (\n",
        "    pd.DataFrame({'locality': X_train['locality'], 'price': y_train})\n",
        "    .groupby('locality')['price']\n",
        "    .agg(\n",
        "        loc_q25=lambda x: x.quantile(0.25),\n",
        "        loc_median='median',\n",
        "        loc_q75=lambda x: x.quantile(0.75)\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Merge into train and test\n",
        "# Merge market context features into TRAIN data\n",
        "\n",
        "X_train = X_train.merge(location_price_stats, on='locality', how='left')\n",
        "\n",
        "# Merge SAME statistics into TEST data\n",
        "# No recomputation → no leakage\n",
        "\n",
        "X_test  = X_test.merge(location_price_stats, on='locality', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkv2NnnB5YB0"
      },
      "source": [
        "## 3a) Leakage Safe Target Label Generation\n",
        "\n",
        " We convert the continuous 'price' variable into categorical labels.\n",
        "\n",
        "   0 = Underpriced (Price is in the bottom 25% of the local market)\n",
        "\n",
        "   1 = Fairly priced (Price is in the middle 50% of the local market)\n",
        "\n",
        "   2 = Overpriced (Price is in the top 25% of the local market)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkyEfeUE5YbT"
      },
      "outputs": [],
      "source": [
        "# Create Classification Labels (Leakage-Safe)\n",
        "\n",
        "# 0 = Underpriced\n",
        "# 1 = Fairly priced\n",
        "# 2 = Overpriced\n",
        "\n",
        "def create_price_class(price, q25, q75):\n",
        "    if price < q25:\n",
        "        return 0\n",
        "    elif price > q75:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "y_train_cls = [\n",
        "    create_price_class(p, q25, q75)\n",
        "    for p, q25, q75 in zip(y_train, X_train['loc_q25'], X_train['loc_q75'])\n",
        "]\n",
        "\n",
        "y_test_cls = [\n",
        "    create_price_class(p, q25, q75)\n",
        "    for p, q25, q75 in zip(y_test, X_test['loc_q25'], X_test['loc_q75'])\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLbSbmYh9mx8"
      },
      "source": [
        "## 3b) Feature Engineering ( Classification Aligned)\n",
        "\n",
        "We create contextual features that translate raw data into \"Appraisal Logic.\"\n",
        "Note: All statistics (Medians, Means, Counts) are derived ONLY from Training data to maintain a strict wall against data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ4YYF-S9nUM"
      },
      "outputs": [],
      "source": [
        "# These features encode RELATIVE price position and structural context, which align directly with: Underpriced / Fair / Overpriced classification\n",
        "# Price (y) is used ONLY because target is derived from price\n",
        "# No future or test aggregation is performed\n",
        "# Relative price position (contextual, NOT residual-based)\n",
        "# Measures how far a listing deviates from its local market\n",
        "X_train['price_position'] = (y_train - X_train['loc_median']) / X_train['loc_median']\n",
        "X_test['price_position']  = (y_test  - X_test['loc_median'])  / X_test['loc_median']\n",
        "\n",
        "\n",
        "# Structural price normalization\n",
        "# Captures whether a property is structurally above or below its local norm\n",
        "X_train['price_per_bedroom']  = y_train / (X_train['bedrooms'] + 1)\n",
        "X_test['price_per_bedroom']   = y_test  / (X_test['bedrooms'] + 1)\n",
        "\n",
        "X_train['price_per_bathroom'] = y_train / (X_train['bathrooms'] + 1)\n",
        "X_test['price_per_bathroom']  = y_test  / (X_test['bathrooms'] + 1)\n",
        "\n",
        "\n",
        "# Structural deviation vs local average\n",
        "location_structure_stats = (\n",
        "    X_train\n",
        "    .groupby('locality')[['bedrooms', 'bathrooms']]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .rename(columns={\n",
        "        'bedrooms': 'loc_avg_bedrooms',\n",
        "        'bathrooms': 'loc_avg_bathrooms'\n",
        "    })\n",
        ")\n",
        "\n",
        "# Merge TRAIN statistics into both sets\n",
        "X_train = X_train.merge(location_structure_stats, on='locality', how='left')\n",
        "X_test  = X_test.merge(location_structure_stats, on='locality', how='left')\n",
        "\n",
        "X_train['bedroom_deviation']  = X_train['bedrooms']  - X_train['loc_avg_bedrooms']\n",
        "X_test['bedroom_deviation']   = X_test['bedrooms']   - X_test['loc_avg_bedrooms']\n",
        "\n",
        "# Deviation features\n",
        "X_train['bathroom_deviation'] = X_train['bathrooms'] - X_train['loc_avg_bathrooms']\n",
        "X_test['bathroom_deviation']  = X_test['bathrooms']  - X_test['loc_avg_bathrooms']\n",
        "\n",
        "\n",
        "# Market density (thin vs thick markets)\n",
        "# Approximates market liquidity:\n",
        "# High density → stable pricing\n",
        "# Low density → volatile pricing\n",
        "location_density = X_train['locality'].value_counts(normalize=True)\n",
        "\n",
        "X_train['location_density'] = X_train['locality'].map(location_density)\n",
        "X_test['location_density']  = X_test['locality'].map(location_density)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK298PJ-_VMW"
      },
      "source": [
        "## 3c) Drop Leakage Prone and Redundant columns\n",
        "\n",
        "We now remove the raw benchmark columns and high-cardinality strings. Reasons for dropping:\n",
        "\n",
        " 1. LEAKAGE: loc_q25/75 were used to define the Target. Including them in X would allow the model to \"cheat\" by learning the labeling rule.\n",
        " 2. REDUNDANCY: The information in loc_avg_bedrooms/bathrooms is already captured in our 'deviation' features.\n",
        " 3. OVERFITTING: Raw 'locality' names lead to high-dimensionality; we use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8THxEfo1_Vhl"
      },
      "outputs": [],
      "source": [
        "# The following columns were used ONLY as intermediate\n",
        "# variables for:\n",
        "# - label creation\n",
        "# - contextual feature engineering\n",
        "#\n",
        "# Keeping them would allow the model to:\n",
        "# - memorize market boundaries\n",
        "# - indirectly reconstruct the target labels\n",
        "#\n",
        "# Therefore, they are removed before modeling.\n",
        "# --------------------------------------------------\n",
        "drop_cols = [\n",
        "    'locality',\n",
        "    'loc_q25',\n",
        "    'loc_q75',\n",
        "    'loc_median',\n",
        "    'loc_avg_bedrooms',\n",
        "    'loc_avg_bathrooms'\n",
        "]\n",
        "\n",
        "X_train_final = X_train.drop(columns=drop_cols)\n",
        "X_test_final  = X_test.drop(columns=drop_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY74w43ZBYYd"
      },
      "source": [
        "## 3d) Automated Feature Type Detection\n",
        "\n",
        "We segregate features by their data types to ensure they receive the correct preprocessing in the Scikit-Learn Pipeline.\n",
        "    1. Categorical: Needs Encoding (e.g., One-Hot)\n",
        "    2. Numerical: Needs Scaling (e.g., Standard Scaling)\n",
        "\n",
        "Automatically identify Categorical columns (Strings/Objects/Categories)\n",
        "Common examples in this dataset: 'type', 'category', 'furnished'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmgDa10Hptcw"
      },
      "outputs": [],
      "source": [
        "# Define Feature Types for Pipeline\n",
        "\n",
        "categorical_features = X_train_final.select_dtypes(\n",
        "    include=['object', 'category']\n",
        ").columns.tolist()\n",
        "\n",
        "numerical_features = X_train_final.select_dtypes(\n",
        "    include=['int64', 'float64']\n",
        ").columns.tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TukBXLwjCFbX"
      },
      "source": [
        "## 3e) PreProcessing\n",
        "\n",
        "We define a multi-path pipeline to handle different data types simultaneously. This ensures that our preprocessing is consistent across Train and Test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSKOiTNsp1vw"
      },
      "outputs": [],
      "source": [
        "# Preprocessing Pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')), # Impute NaNs for numerical features\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
        "         categorical_features)\n",
        "    ]\n",
        ")"
      ]
    }
  ]
}